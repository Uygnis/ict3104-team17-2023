# -*- coding: utf-8 -*-
"""Full Flow of FYP [DO NOT TOUCH].ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1XdgEihd99v_wrNxGctVIwOyXiJsa4x6i

#ICT3104 Team 17

This project makes use of the existing Follow Your Pose project and curated it in such a way that users are able to upload their video files locally for both Training and MMPose. Users should also be able to compare the outputs  of the training to decide which outputs suit best for them. In the first section,  users would run MMPose code, followed by training then Inferencing.

## 1. MMPose
"""

# Commented out IPython magic to ensure Python compatibility.
#@title 1.1 Install Dependencies
!sudo apt update -y
!sudo apt install python3.8 python3.8-distutils python3.8-dev -y
!update-alternatives --install /usr/local/bin/python3 python3 /usr/bin/python3.8 2
!python --version
!apt-get update
!apt install software-properties-common
!sudo dpkg --remove --force-remove-reinstreq python3-pip python3-setuptools python3-wheel
!apt-get install python3-pip

print('Git clone project and install requirements...')
!git clone https://github.com/Uygnis/ict3104-team17-2023.git

!cd /content/ict3104-team17-2023
!export PYTHONPATH=/content/ict3104-team17-2023:$PYTHONPATH
!apt update
!python3.8 -m pip install -q diffusers==0.11.1 torch==1.13.1 transformers==4.26.0 bitsandbytes==0.35.4 \
decord accelerate einops imageio-ffmpeg xformers==0.0.16  --extra-index-url https://download.pytorch.org/whl/cu11
# %pip install -q omegaconf
# %cd /content/ict3104-team17-2023
!python3.8 -m pip install -r requirements.txt

!pip install moviepy jsonlines pytube ipywidgets IPython PyYAML
!pip install jsonlines
!pip uninstall triton -y
!pip install triton==2.0.0 -U

#@title 1.2 Upload MMPose Input Video [OPTIONAL]

#@markdown Upload your video by running this cell.

#@markdown OR

#@markdown You can use the file manager on the left panel to upload (drag and drop) to `data` folder.

import os
from google.colab import files
import shutil

uploaded = files.upload()
for filename in uploaded.keys():
    dst_path = os.path.join("./mmpose_input", filename)
    shutil.move(filename, dst_path)

#@title 1.3 Create Pose Generation Script & Run

Clip_Length = 36 #@param {type:"number"}
Clip_Height = 512 #@param {type:"number"}

#@markdown Change the file name if you upload your own file (i.e jumping1_full.mp4)
Input_Path = "./mmpose_input/jumping1_full.mp4" #@param {type:"string"}
Output_Filename = "output_file.mp4" #@param {type:"string"}

python_code="""import os
import cv2
import numpy as np
from PIL import Image
from moviepy.editor import *
import copy
import gradio as gr
from transformers import AutoTokenizer, CLIPTextModel
from huggingface_hub import snapshot_download
import sys
import argparse

sys.path.append('FollowYourPose')

def get_frames(video_in):
    print("Working on:" + video_in)
    frames = []
    #resize the video
    clip = VideoFileClip(video_in)
    start_frame = 0
    end_frame = {}
    clip_height = {}


    if not os.path.exists('./raw_frames'):
        os.makedirs('./raw_frames')

    if not os.path.exists('./mmpose_frames'):
        os.makedirs('./mmpose_frames')


    clip_resized = clip.resize(height=clip_height)
    print("Resized Height")
    clip_resized = clip_resized.subclip(start_frame / clip.fps, end_frame / clip.fps) # subclip 5 seconds
    print("Cut the Clip")
    clip_resized.write_videofile("./video_resized.mp4", fps=clip.fps)
    end_frame = int(clip.fps * clip_resized.duration)

    # Opens the Video file with CV2
    cap= cv2.VideoCapture("./video_resized.mp4")

    fps = int(cap.get(cv2.CAP_PROP_FPS))
    print("video fps: " + str(fps))
    i=0
    while(cap.isOpened()):
        ret, frame = cap.read()
        if ret == False:
            break
        cv2.imwrite('./raw_frames/kang'+str(i)+'.jpg',frame)
        frames.append('./raw_frames/kang'+str(i)+'.jpg')
        i+=1

    cap.release()
    cv2.destroyAllWindows()
    print("broke the video into frames")

    return frames, fps

def get_mmpose_filter(mmpose, i):
    #image = Image.open(i)

    #image = np.array(image)
    image = mmpose(i, fn_index=0)[1]
    image = Image.open(image)
    #image = Image.fromarray(image)
    image.save("./mmpose_frames/mmpose_frame_" + str(i).split('/')[-1][:-4] + ".jpeg")
    return "./mmpose_frames/mmpose_frame_" + str(i).split('/')[-1][:-4] + ".jpeg"

def create_video(frames, fps,  output_directory="./mmpose_output"):
    # Create the output directory if it doesn't exist
    os.makedirs(output_directory, exist_ok=True)

    print("building video result")
    clip = ImageSequenceClip(frames, fps=fps)
    output_filename = f"{}"
    output_path = os.path.join(output_directory, output_filename)
    clip.write_videofile(output_path, fps=fps)

    return output_path


def infer_skeleton(mmpose, video_in):


    # 1. break video into frames and get FPS

    break_vid = get_frames(video_in)
    frames_list= break_vid[0]
    fps = break_vid[1]
    #n_frame = int(trim_value*fps)
    n_frame = len(frames_list)

    if n_frame >= len(frames_list):
        print("video is shorter than the cut value")
        n_frame = len(frames_list)

    # 2. prepare frames result arrays
    result_frames = []
    print("set stop frames to: " + str(n_frame))

    for i in frames_list[0:int(n_frame)]:
        mmpose_frame = get_mmpose_filter(mmpose, i)
        result_frames.append(mmpose_frame)
        print("frame " + i + "/" + str(n_frame) + ": done;")


    final_vid = create_video(result_frames, fps, "mmpose_output")
    files = [final_vid]

    return final_vid, files


mmpose = gr.Interface.load(name="spaces/YueMafighting/mmpose-estimation")
final_vid, files = infer_skeleton(mmpose, "{}")""".format(Clip_Length, Clip_Height, Output_Filename,Input_Path)

with open("poseGeneration.py", 'w') as file:
    file.write(python_code)

#run the script
!python3.8 poseGeneration.py

"""## 2. Training"""

# Commented out IPython magic to ensure Python compatibility.
#@title 2.1 Download Pretrained Models

#@markdown Name/Path of the initial model.
MODEL_DIR = "FollowYourPose_v1" #@param {type:"string"}

#@markdown If model should be download from a remote repo. Untick it if the model is loaded from a local path.
download_pretrained_model = True #@param {type:"boolean"}
if download_pretrained_model:
#     %mkdir checkpoints
    !git lfs install
    !git clone https://huggingface.co/YueMafighting/$MODEL_DIR
#     %mv /content/ict3104-team17-2023/FollowYourPose_v1/* /content/ict3104-team17-2023/checkpoints/
#     %rm -rf FollowYourPose_v1
#     %mkdir output
#     %mkdir output/followyourpose_checkpoint-1000
#     %mv /content/ict3104-team17-2023/checkpoints/followyourpose_checkpoint-1000/* /content/ict3104-team17-2023/output/followyourpose_checkpoint-1000
#     %rm -rf checkpoints/followyourpose_checkpoint-1000
    MODEL_DIR = f"./checkpoints"
print(f"[*] MODEL_DIR={MODEL_DIR}")

#@title 2.2 Upload Your Own Dataset [OPTIONAL]

# Import the necessary libraries
import os
from google.colab import files
import shutil

# Define the parameters using param
folder_name = "CustomFolder"  # Default folder name
create_new_folder = True  # Default choice to create a new folder

#@title Upload Your Dataset

#@markdown Enter the name of the folder to create and upload to:
folder_name = "" #@param {type:"string"}

#@markdown Check this box to create a new folder for the uploaded dataset:
create_new_folder = True #@param {type:"boolean"}

# Define the target folder path
if create_new_folder:
    # Create a new folder with the specified name inside the "dataset" directory
    new_folder = f"./dataset/{folder_name}"
    if not os.path.exists(new_folder):
        os.makedirs(new_folder)
else:
    # Use the existing "dataset" directory
    new_folder = "./dataset"

# Upload the file(s) and move them to the specified folder
uploaded = files.upload()
for filename in uploaded.keys():
    # Check if the file with the same name already exists in the folder
    if os.path.exists(os.path.join(new_folder, filename)):
        # Append a unique identifier to the filename
        base, ext = os.path.splitext(filename)
        count = 1
        while os.path.exists(os.path.join(new_folder, f"{base}_{count}{ext}")):
            count += 1
        new_filename = f"{base}_{count}{ext}"
    else:
        new_filename = filename

    dst_path = os.path.join(new_folder, new_filename)
    shutil.move(filename, dst_path)

# Print the path to the folder where the files were uploaded
print(f"Uploaded files are located in the folder: {new_folder}")

# Commented out IPython magic to ensure Python compatibility.
# # Import the necessary libraries
# 
# %%writefile download.py
# import os
# import jsonlines
# from pytube import YouTube
# from moviepy.editor import VideoFileClip
# 
# # Define the parameters using param
# dataset_directory = './dataset'  # Default dataset directory
# jsonl_file_name = 'hdvila.jsonl'  # Default JSONL file name
# output_directory_downloaded = './dataset/hdvila/downloaded_videos'  # Default downloaded videos directory
# output_directory_clips = './dataset/hdvila/video_clips'  # Default video clips directory
# 
# #@title Generate HDVila Dataset [OPTIONAL]
# 
# #@markdown Specify the dataset directory:
# dataset_directory = './dataset/hdvila' #@param {type:"string"}
# 
# #@markdown Specify the name of the JSONL file (e.g., hdvila.jsonl):
# jsonl_file_name = 'hdvila_part1(6).jsonl' #@param {type:"string"}
# 
# #@markdown Specify the directory for downloaded videos:
# output_directory_downloaded = './dataset/hdvila/downloaded_videos' #@param {type:"string"}
# 
# #@markdown Specify the directory for video clips:
# output_directory_clips = './dataset/hdvila/video_clips' #@param {type:"string"}
# 
# # Define the JSONL file path
# jsonl_file = os.path.join(dataset_directory, jsonl_file_name)
# 
# # Check if a JSONL file was found
# if os.path.exists(jsonl_file):
#     print(f"Using JSONL file: {jsonl_file}")
# 
#     def download_videos_from_jsonl(jsonl_file, output_directory):
#         # Create the output directory if it doesn't exist
#         os.makedirs(output_directory, exist_ok=True)
# 
#         with jsonlines.open(jsonl_file, 'r') as reader:
#             for line in reader:
#                 video_url = line['url']
#                 video_id = line['video_id']
#                 output_file = os.path.join(output_directory, f"{video_id}.mp4")
# 
#                 try:
#                     # Create a YouTube object with the provided URL
#                     yt = YouTube(video_url)
# 
#                     streams = yt.streams.filter(res="480p")
# 
#                     if streams:
#                         # Get the first stream in the list (480p stream)
#                         video_stream = streams[0]
# 
#                         # Download the video
#                         video_stream.download(output_path=output_directory, filename=f"{video_id}.mp4")
# 
#                         print(f"Video {video_id} downloaded successfully in 480p resolution.")
#                     else:
#                         print("No 480p stream available for this video.")
#                 except Exception as e:
#                     print(f"An error occurred while downloading {video_id}: {str(e)}")
# 
#     # Define the output directory for downloaded videos
#     os.makedirs(output_directory_downloaded, exist_ok=True)
# 
#     # Download videos from the JSONL file using pytube
#     download_videos_from_jsonl(jsonl_file, output_directory_downloaded)
# 
#     def calculate_num_segments(clip_data):
#         return len(clip_data)
# 
#     def resize_and_split_video(video_path, video_id, clip_data, output_directory, target_resolution=(512, 512), target_fps=480):
#         try:
#             num_segments = calculate_num_segments(clip_data)
#             for i, clip in enumerate(clip_data):
#                 span = clip['span']
#                 start_time = float(span[0].split(':')[-1])
#                 end_time = float(span[1].split(':')[-1])
#                 directory_path = f'./{output_directory}/{video_id}/video_clips'
# 
#                 # Check if the directory exists
#                 if not os.path.exists(directory_path):
#                     # Create the directory if it doesn't exist
#                     os.makedirs(directory_path)
#                     print(f"Directory '{directory_path}' created.")
# 
#                 output_file = f"{output_directory}/{video_id}/video_clips/{i}.mp4"
# 
#                 video = VideoFileClip(video_path).subclip(start_time, end_time)
# 
#                 # Resize the video segment to the target resolution
#                 video = video.resize(target_resolution)
# 
#                 # Set the target frame rate
#                 video = video.set_fps(target_fps)
# 
#                 video.write_videofile(output_file, codec='libx264')
# 
#                 print(f"Segment {i+1} of {num_segments} for {video_id}: {output_file} created successfully.")
#         except Exception as e:
#             print(f"An error occurred: {str(e)}")
# 
#     # Define the output directory for video clips
#     os.makedirs(output_directory_clips, exist_ok=True)
# 
#     # Open the JSONL file and process each video entry
#     with jsonlines.open(jsonl_file, 'r') as reader:
#         for line in reader:
#             video_id = line['video_id']
#             video_url = line['url']
#             clip_data = line['clip']
# 
#             # Construct the full video file path including the ".mp4" extension
#             video_path = os.path.join(output_directory_downloaded, f"{video_id}.mp4")
# 
#             # Call the resize_and_split_video function to resize and split the clip
#             resize_and_split_video(video_path, video_id, clip_data, output_directory_clips)
# 
# else:
#     print(f"JSONL file not found in the dataset directory: {jsonl_file}")
# 
# 
#

!python3.8 download.py

#@title specify file dirs
#@markdown Pretrained Model Path
STABLE_DIFFUSION_MODEL = "stable-diffusion-v1-4" #@param {type:"string"}
STABLE_DIFFUSION_MODEL = f"{MODEL_DIR}/{STABLE_DIFFUSION_MODEL}"
print(f"[*] STABLE_DIFFUSION_MODEL={STABLE_DIFFUSION_MODEL}")

#@markdown Output Path
OUTPUT_DIR = "output" #@param {type:"string"}
OUTPUT_DIR = f"./{OUTPUT_DIR}"
print(f"[*] OUTPUT_DIR={OUTPUT_DIR}")

#@markdown Checkpoint Model Path
CHECKPOINT_MODEL = "apose2-checkpoint-1050"  #@param {type:"string"}
CHECKPOINT_MODEL = f"{OUTPUT_DIR}/{CHECKPOINT_MODEL}"
print(f"[*] CHECKPOINT_MODEL={CHECKPOINT_MODEL}")

#@markdown Config Path
CONFIG_DIR = "configs" #@param {type:"string"}
CONFIG_DIR = f"./{CONFIG_DIR}"
print(f"[*] CONFIG_DIR={CONFIG_DIR}")

#@markdown Specify the directory where you want to save skeleton .mp4 files
SKELETON_DATA = "mmpose_output" #@param {type:"string"}
SKELETON_DATA = f"./{SKELETON_DATA}"
print(f"[*] SKELETON_DATA={SKELETON_DATA}")

#@markdown Specify the directory where you want to search for .mp4 files
VIDEO_DATA = "output_file.mp4" #@param {type:"string"}
VIDEO_DATA = f"./dataset/{VIDEO_DATA}/video_clips"
print(f"[*] VIDEO_DATA={VIDEO_DATA}")

# Commented out IPython magic to ensure Python compatibility.
#@title Train config for fine-tuning
from omegaconf import OmegaConf

# %mkdir configs
def checkArr(variable, count): # checks if batch
  if len(variable.split(",")) > 1:
    return count
  return 0


#@markdown
#@markdown use comma(,) to signify BATCH. Make sure all have same array length
CONFIG_NAME = "configs/apose.yaml,configs/apose1.yaml,configs/apose2.yaml" #@param {type:"string"}
video_length = "4,4,4" #@param {type:"string"}
sample_frame_rate = "4,4,4" #@param {type:"string"}
train_steps = "1050,1050,1050" #@param {type:"string"}
checkpointing_steps="1050,1050,1050" #@param {type:"string"}
validation_steps= "1050,1050,1050" #@param {type:"string"}

#@markdown
#@markdown UNBATCH
learning_rate = 3e-5 #@param {type:"number"}
width = 512 #@param {type:"number"}
height = 512 #@param {type:"number"}
validation_prompts = "A Iron man on the beach," # @param {type:"string"}
skeleton_path = './mmpose_output/output_file.mp4' # @param {type:"string"}
train_prompt = "A person walks into a garage and throws something into a bin. That same person then picks up an open laptop off a shelf, closes it and walks way.;A person runs into the garage, throws something in a bucket, picks up a laptop from a shelf, closes it and then runs back out." #@param {type:"string"}
meta_path = "Charades_v1_desc.csv" # @param {type:"string"}

for i in range(len(CONFIG_NAME.split(","))):
  OUTPUT = OUTPUT_DIR
  config = {
    "pretrained_model_path": STABLE_DIFFUSION_MODEL,
    "output_dir": OUTPUT_DIR,
    "resume_from_checkpoint": CHECKPOINT_MODEL,
    "skeleton_path": skeleton_path,
    "config_name":CONFIG_NAME.split(",")[i].split("/")[1],
    "train_data": {
      "meta_path":meta_path,
      "prompt": train_prompt,
      "n_sample_frames": 4,
      "width": width,
      "height": height,
      "sample_start_idx": 0,
      "dataset_set": "train",
      "sample_frame_rate":  int(sample_frame_rate.split(',')[i]),
    },
    "validation_data": {
      "prompts":
        validation_prompts.split(", ")
      ,
      "video_length": int(video_length.split(',')[i]),
      "width": width,
      "height": height,
      "num_inference_steps": 20,
      "guidance_scale": 12.5,
      "use_inv_latent": False,
      "num_inv_steps": 20,
      "dataset_set": "val"
    },
    "learning_rate": learning_rate,
    "train_batch_size": 1,
    "max_train_steps":   int(train_steps.split(',')[i]),
    "checkpointing_steps":  int(checkpointing_steps.split(',')[i]),
    "validation_steps": int(validation_steps.split(',')[i]),
    "trainable_modules": [
      "attn1.to_q",
      "attn2.to_q",
      "attn_temp",
      "conv_temporal",
    ],
    "seed": 33,
    "mixed_precision": "no",
    "use_8bit_adam": False,
    "gradient_checkpointing": True,
    "enable_xformers_memory_efficient_attention": True,
  }

  OmegaConf.save(config, CONFIG_NAME.split(",")[i])

cat ./configs/apose.yaml

#@title 2.5 Adjust Frame Dimension
import glob
import cv2

# Use glob to get a list of .mp4 files in the specified directory
mp4_files = glob.glob(f"{VIDEO_DATA}/*.mp4")

# Print the list of .mp4 files
for mp4_file in mp4_files:
    cap = cv2.VideoCapture(mp4_file)
    frame_width = 512  #@param {type:"number"}
    original_frame_width = int(cap.get(3))
    original_frame_height = int(cap.get(4))
    # Calculate the new frame height while maintaining the aspect ratio
    frame_height = int(frame_width * original_frame_height / original_frame_width)

    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(mp4_file, fourcc, 30, (frame_width, frame_height))
    while True:
      ret, frame = cap.read()
      if not ret:
          break
      frame = cv2.resize(frame, (frame_width, frame_height))
      out.write(frame)

    cap.release()
    out.release()
    print(mp4_file, " has been cropped")

#@title 2.6 Upload Your Own Skeleton [OPTIONAL]

#@markdown Upload your video by running this cell.

#@markdown OR

#@markdown You can use the file manager on the left panel to upload (drag and drop) to `data` folder.

import os
from google.colab import files
import shutil

uploaded = files.upload()
for filename in uploaded.keys():
    dst_path = os.path.join("./mmpose_output", filename)
    shutil.move(filename, dst_path)

#@title 2.7 Create Dataloader File
folder_name = "Charades_v1_480" #@param {type:"string"}
custom_folder_name = folder_name
content="""import os
import random
from abc import abstractmethod
import math
import pandas as pd
import av
import cv2
import decord
import numpy as np
import torch
import json
import jsonlines
from IPython.display import display, HTML
from PIL import Image
from torch.utils.data import Dataset
from torchvision import transforms
from decord import VideoReader, cpu
import torchvision.transforms._transforms_video as transforms_video
from torchvision.transforms.functional import to_tensor
from collections import OrderedDict
import time
import csv
import base64


class Dataset(Dataset):
    \"\"\"
    Dataset.
    Assumes webvid data is structured as follows.
    HDVila/
        part_N/ # 0-10
            video_clips/        ($page_dir)
                1.mp4           (videoid.mp4)
                ...
                5000.mp4
            ...
    \"\"\"



    def __init__(self,
                meta_path,
                width=512,
                height=512,
                n_sample_frames=8,
                dataset_set="train",
                prompt=None,
                sample_frame_rate=4,
                sample_start_idx=0,
                accelerator=None,
                ):
        try:
            host_gpu_num = accelerator.num_processes
            host_num = 1
            all_rank = host_gpu_num * host_num
            global_rank = accelerator.local_process_index
        except:
            pass

        self.data_dir = './dataset'
        self.meta_path = os.path.join(self.data_dir, meta_path)

        spatial_transform = 'resize_center_crop'
        resolution=width
        load_raw_resolution=True

        video_length= n_sample_frames
        fps_max=None
        load_resize_keep_ratio=False



        self.global_rank = global_rank
        self.all_rank = all_rank
        # self.subsample = subsample
        self.video_length = video_length
        self.resolution = [resolution, resolution] if isinstance(resolution, int) else resolution
        self.frame_stride = sample_frame_rate
        self.load_raw_resolution = load_raw_resolution
        self.fps_max = fps_max
        self.load_resize_keep_ratio = load_resize_keep_ratio
        print('start load meta data')
        self._load_metadata()
        print('load meta data done!!!')
        if spatial_transform is not None:
            if spatial_transform == "random_crop":
                self.spatial_transform = transforms_video.RandomCropVideo(crop_resolution)
            elif spatial_transform == "resize_center_crop":
                assert(self.resolution[0] == self.resolution[1])
                self.spatial_transform = transforms.Compose([
                    transforms.Resize(resolution),
                    transforms_video.CenterCropVideo(resolution),
                    ])
                print(self.spatial_transform)
            elif spatial_transform == "center_crop":
                self.spatial_transform = transforms_video.CenterCropVideo(resolution)
            else:
                raise NotImplementedError
        else:
            self.spatial_transform = None



    def _load_metadata(self):
        # clip_id frame_id caption
        self.metadata = []
        start_time = time.time()
        # Define path to meta data
        metadata_path = self.meta_path

        try:
          # Open excel file and load contents
          with open(metadata_path, 'r',encoding="utf-8") as csvfile: #41s
              reader = csv.DictReader(csvfile)
              for row in reader:
                  if row['id']:
                      self.metadata.append([row['id']])
                      self.metadata[-1].append([row['descriptions']])
                  print("Loading video:", row['id'])
        except Exception as e:
            print(f"Error loading metadata from excel file: {{str(e)}}")
        end_time = time.time()
        print('load %d items use time: %.1f;' % (len(self.metadata), end_time-start_time))


    def _get_video_path(self, sample):
        folder_name = "{}"
        clip_id = sample[0]
        video_path = os.path.join(self.data_dir, folder_name, 'video_clips', f'{{clip_id}}.mp4')    # ['Good_1Oiumw', '0.mp4', '00:00:01.669 to 00:01:26.679']
        print("Video Path: ", video_path)
        return video_path

    def __getitem__(self, index):
        while True:

            index = index % len(self.metadata)
            sample = self.metadata[index]
            print(sample)
            video_path = self._get_video_path(sample)

            try:
                if self.load_raw_resolution:
                    video_reader = VideoReader(video_path, ctx=cpu(0))
                elif self.load_resize_keep_ratio:
                    # resize scale is according to the short side
                    h, w, c = VideoReader(video_path, ctx=cpu(0))[0].shape
                    if h < w:
                        scale = h / self.resolution[0]
                    else:
                        scale = w / self.resolution[1]

                    h = math.ceil(h / scale)
                    w = math.ceil(w / scale)
                    video_reader = VideoReader(video_path, ctx=cpu(0), width=w, height=h)
                else:
                    video_reader = VideoReader(video_path, ctx=cpu(0), width=self.resolution[1], height=self.resolution[0])
                if len(video_reader) < self.video_length:
                    print(f"video length ({{len(video_reader)}}) is smaller than target length({{self.video_length}})")
                    index += 1
                    continue
                else:
                    pass
            except:
                index += 1
                print(f"Load video failed! path = {{video_path}}")
                continue
            fps_ori = video_reader.get_avg_fps()

            fs = self.frame_stride
            allf = len(video_reader)
            if self.frame_stride != 1:
                all_frames = list(range(0, len(video_reader), self.frame_stride))
                if len(all_frames) < self.video_length:
                    fs = len(video_reader) // self.video_length
                    assert(fs != 0)
                    all_frames = list(range(0, len(video_reader), fs))
            else:
                all_frames = list(range(len(video_reader)))

            # select a random clip
            rand_idx = random.randint(0, len(all_frames) - self.video_length)
            frame_indices = all_frames[rand_idx:rand_idx+self.video_length]
            try:
                frames = video_reader.get_batch(frame_indices)
                break
            except:
                print(f"Get frames failed! path = {{video_path}}")
                index += 1
                continue

        assert(frames.shape[0] == self.video_length),f'{{len(frames)}}, self.video_length={{self.video_length}}'
        frames = torch.tensor(frames.asnumpy()).permute(3, 0, 1, 2).float() # [t,h,w,c] -> [c,t,h,w]

        if self.spatial_transform is not None:
            frames = self.spatial_transform(frames)
        assert(frames.shape[2] == self.resolution[0] and frames.shape[3] == self.resolution[1]), f'frames={{frames.shape}}, self.resolution={{self.resolution}}'
        frames = frames.byte()
        # fps
        fps_clip = fps_ori // self.frame_stride
        if self.fps_max is not None and fps_clip > self.fps_max:
            fps_clip = self.fps_max

        caption = sample[1][0]

        frames = frames.permute(1,0,2,3)
        skeleton_final = torch.zeros_like(frames).byte()
        frames = (frames / 127.5 - 1.0)
        skeleton_final = (skeleton_final / 127.5 - 1.0)
        example = {{'pixel_values': frames, 'sentence': caption, 'pose': skeleton_final}}

        return example

    def __len__(self):
        return len(self.metadata)"""

content = content.format(custom_folder_name)

with open("./followyourpose/data/dataloader.py", 'w') as file:
    file.write(content)

#run the script
!python3.8 ./followyourpose/data/dataloader.py

# Commented out IPython magic to ensure Python compatibility.
# #@title 2.8 Create Training File
# %%writefile trainFYP.py
# import argparse
# import datetime
# import logging
# import inspect
# import yaml
# import math
# import os
# from typing import Dict, Optional, Tuple
# from omegaconf import OmegaConf
# 
# import torch
# import torch.nn.functional as F
# import torch.utils.checkpoint
# 
# import diffusers
# import transformers
# from accelerate import Accelerator
# from accelerate.logging import get_logger
# from accelerate.utils import set_seed
# from diffusers import AutoencoderKL, DDPMScheduler, DDIMScheduler
# from diffusers.optimization import get_scheduler
# from diffusers.utils import check_min_version
# from diffusers.utils.import_utils import is_xformers_available
# from tqdm.auto import tqdm
# from transformers import CLIPTextModel, CLIPTokenizer
# 
# from followyourpose.models.unet import UNet3DConditionModel
# from followyourpose.data.dataloader import Dataset
# from followyourpose.pipelines.pipeline_followyourpose import FollowYourPosePipeline
# from followyourpose.util import save_videos_grid, ddim_inversion
# from einops import rearrange
# 
# 
# # Will error if the minimal version of diffusers is not installed. Remove at your own risks.
# check_min_version("0.10.0.dev0")
# 
# logger = get_logger(__name__, log_level="INFO")
# 
# 
# def main(
#     pretrained_model_path: str,
#     config_name: str,
#     output_dir: str,
#     train_data: Dict,
#     validation_data: Dict,
#     validation_steps: int = 100,
#     trainable_modules: Tuple[str] = (
#         "attn1.to_q",
#         "attn2.to_q",
#         "attn_temp",
#     ),
#     train_batch_size: int = 1,
#     max_train_steps: int = 500,
#     learning_rate: float = 3e-2,
#     scale_lr: bool = False,
#     lr_scheduler: str = "constant",
#     lr_warmup_steps: int = 0,
#     adam_beta1: float = 0.9,
#     adam_beta2: float = 0.999,
#     adam_weight_decay: float = 1e-2,
#     adam_epsilon: float = 1e-08,
#     max_grad_norm: float = 1.0,
#     gradient_accumulation_steps: int = 1,
#     gradient_checkpointing: bool = True,
#     checkpointing_steps: int = 500,
#     resume_from_checkpoint: Optional[str] = None,
#     mixed_precision: Optional[str] = "fp16",
#     use_8bit_adam: bool = False,
#     enable_xformers_memory_efficient_attention: bool = True,
#     seed: Optional[int] = None,
#     skeleton_path: Optional[str] = None,
# ):
#     *_, config = inspect.getargvalues(inspect.currentframe())
# 
#     accelerator = Accelerator(
#         gradient_accumulation_steps=gradient_accumulation_steps,
#         mixed_precision=mixed_precision,
#     )
# 
#     # Make one log on every process with the configuration for debugging.
#     logging.basicConfig(
#         format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
#         datefmt="%m/%d/%Y %H:%M:%S",
#         level=logging.INFO,
#     )
#     logger.info(accelerator.state, main_process_only=False)
#     if accelerator.is_local_main_process:
#         transformers.utils.logging.set_verbosity_warning()
#         diffusers.utils.logging.set_verbosity_info()
#     else:
#         transformers.utils.logging.set_verbosity_error()
#         diffusers.utils.logging.set_verbosity_error()
# 
#     # If passed along, set the training seed now.
#     if seed is not None:
#         set_seed(seed)
# 
#     # Handle the output folder creation
#     if accelerator.is_main_process:
# 
#         os.makedirs(output_dir, exist_ok=True)
#         os.makedirs(f"{output_dir}/samples", exist_ok=True)
#         os.makedirs(f"{output_dir}/inv_latents", exist_ok=True)
#         OmegaConf.save(config, os.path.join(output_dir, 'config.yaml'))
# 
#     # Load scheduler, tokenizer and models.
#     noise_scheduler = DDPMScheduler.from_pretrained(pretrained_model_path, subfolder="scheduler")
#     tokenizer = CLIPTokenizer.from_pretrained(pretrained_model_path, subfolder="tokenizer")
#     text_encoder = CLIPTextModel.from_pretrained(pretrained_model_path, subfolder="text_encoder")
#     vae = AutoencoderKL.from_pretrained(pretrained_model_path, subfolder="vae")
#     unet = UNet3DConditionModel.from_pretrained_2d(pretrained_model_path, subfolder="unet")
# 
#     # Freeze vae and text_encoder
#     vae.requires_grad_(False)
#     text_encoder.requires_grad_(False)
# 
#     unet.requires_grad_(False)
#     for name, module in unet.named_modules():
#         if name.endswith(tuple(trainable_modules)):
#             for params in module.parameters():
#                 params.requires_grad = True
# 
#     if enable_xformers_memory_efficient_attention:
#         if is_xformers_available():
#             unet.enable_xformers_memory_efficient_attention()
#         else:
#             raise ValueError("xformers is not available. Make sure it is installed correctly")
# 
#     if gradient_checkpointing:
#         unet.enable_gradient_checkpointing()
# 
#     if scale_lr:
#         learning_rate = (
#             learning_rate * gradient_accumulation_steps * train_batch_size * accelerator.num_processes
#         )
# 
#     # Initialize the optimizer
#     if use_8bit_adam:
#         try:
#             import bitsandbytes as bnb
#         except ImportError:
#             raise ImportError(
#                 "Please install bitsandbytes to use 8-bit Adam. You can do so by running `pip install bitsandbytes`"
#             )
# 
#         optimizer_cls = bnb.optim.AdamW8bit
#     else:
#         optimizer_cls = torch.optim.AdamW
# 
#     optimizer = optimizer_cls(
#         unet.parameters(),
#         lr=learning_rate,
#         betas=(adam_beta1, adam_beta2),
#         weight_decay=adam_weight_decay,
#         eps=adam_epsilon,
#     )
# 
#     # Get the training dataset
#     train_dataset = Dataset(accelerator=accelerator, **train_data)
# 
# 
#     # DataLoaders creation:
#     train_dataloader = torch.utils.data.DataLoader(
#         train_dataset, batch_size=train_batch_size
#     )
# 
#     # Get the validation pipeline
#     validation_pipeline = FollowYourPosePipeline(
#         vae=vae, text_encoder=text_encoder, tokenizer=tokenizer, unet=unet,
#         scheduler=DDIMScheduler.from_pretrained(pretrained_model_path, subfolder="scheduler")
#     )
#     validation_pipeline.enable_vae_slicing()
#     ddim_inv_scheduler = DDIMScheduler.from_pretrained(pretrained_model_path, subfolder='scheduler')
#     ddim_inv_scheduler.set_timesteps(validation_data.num_inv_steps)
# 
#     # Scheduler
#     lr_scheduler = get_scheduler(
#         lr_scheduler,
#         optimizer=optimizer,
#         num_warmup_steps=lr_warmup_steps * gradient_accumulation_steps,
#         num_training_steps=max_train_steps * gradient_accumulation_steps,
#     )
# 
#     # Prepare everything with our `accelerator`.
#     unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
#         unet, optimizer, train_dataloader, lr_scheduler
#     )
# 
#     # For mixed precision training we cast the text_encoder and vae weights to half-precision
#     # as these models are only used for inference, keeping weights in full precision is not required.
#     weight_dtype = torch.float32
#     if accelerator.mixed_precision == "fp16":
#         weight_dtype = torch.float16
#     elif accelerator.mixed_precision == "bf16":
#         weight_dtype = torch.bfloat16
# 
#     # Move text_encode and vae to gpu and cast to weight_dtype
#     text_encoder.to(accelerator.device, dtype=weight_dtype)
#     vae.to(accelerator.device, dtype=weight_dtype)
# 
#     # We need to recalculate our total training steps as the size of the training dataloader may have changed.
#     num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)
#     # Afterwards we recalculate our number of training epochs
#     num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)
# 
#     # We need to initialize the trackers we use, and also store our configuration.
#     # The trackers initializes automatically on the main process.
#     if accelerator.is_main_process:
#         accelerator.init_trackers("text2video-fine-tune")
# 
#     # Train!
#     total_batch_size = train_batch_size * accelerator.num_processes * gradient_accumulation_steps
# 
#     logger.info("***** Running training *****")
#     logger.info(f"  Num examples = {len(train_dataset)}")
#     logger.info(f"  Num Epochs = {num_train_epochs}")
#     logger.info(f"  Instantaneous batch size per device = {train_batch_size}")
#     logger.info(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")
#     logger.info(f"  Gradient Accumulation steps = {gradient_accumulation_steps}")
#     logger.info(f"  Total optimization steps = {max_train_steps}")
#     global_step = 0
#     first_epoch = 0
# 
#     # Potentially load in the weights and states from a previous save
#     if resume_from_checkpoint:
#         if resume_from_checkpoint != "latest":
#             path = os.path.basename(resume_from_checkpoint)
#         else:
#             # Get the most recent checkpoint
#             dirs = os.listdir(output_dir)
#             dirs = [d for d in dirs if d.startswith("checkpoint")]
#             dirs = sorted(dirs, key=lambda x: int(x.split("-")[1]))
#             path = dirs[-1]
#         accelerator.print(f"Resuming from checkpoint {path}")
#         accelerator.load_state(os.path.join(output_dir, path))
#         global_step = int(path.split("-")[1])
#         accelerator.print(f"Gloabl Step: {global_step}")
#         first_epoch = global_step // num_update_steps_per_epoch
#         accelerator.print(f"first_epoch: {first_epoch}")
#         resume_step = global_step % num_update_steps_per_epoch
#         accelerator.print(f"resume_step: {resume_step}")
# 
#     # Only show the progress bar once on each machine.
#     progress_bar = tqdm(range(global_step, max_train_steps), disable=not accelerator.is_local_main_process)
#     progress_bar.set_description("Steps")
# 
# 
# 
#     for epoch in range(first_epoch, num_train_epochs):
#         unet.train()
#         train_loss = 0.0
#         for step, batch in enumerate(train_dataloader):
#             # Skip steps until we reach the resumed step
#             if resume_from_checkpoint and epoch == first_epoch and step < resume_step:
#                 if step % gradient_accumulation_steps == 0:
#                     progress_bar.update(1)
#                 continue
# 
#             with accelerator.accumulate(unet):
#                 # Convert videos to latent space
#                 pixel_values = batch["pixel_values"].to(weight_dtype)
# 
#                 skeleton = batch["pose"].to(weight_dtype)
# 
# 
#                 video_length = pixel_values.shape[1]
# 
#                 pixel_values = rearrange(pixel_values, "b f c h w -> (b f) c h w")
#                 latents = vae.encode(pixel_values).latent_dist.sample()
#                 latents = rearrange(latents, "(b f) c h w -> b c f h w", f=video_length)
#                 latents = latents * 0.18215
# 
#                 # Sample noise that we'll add to the latents
#                 noise = torch.randn_like(latents)
#                 bsz = latents.shape[0]
#                 # Sample a random timestep for each video
#                 timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=latents.device)
#                 timesteps = timesteps.long()
# 
#                 # Add noise to the latents according to the noise magnitude at each timestep
#                 # (this is the forward diffusion process)
#                 noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)
# 
#                 batch["prompt_ids"] = tokenizer(
#                     batch["sentence"], max_length=tokenizer.model_max_length, padding="max_length", truncation=True, return_tensors="pt"
#                 ).input_ids[0].to(accelerator.device).unsqueeze(0)
# 
#                 # Get the text embedding for conditioning
#                 encoder_hidden_states = text_encoder(batch["prompt_ids"])[0]
# 
#                 # Get the target for loss depending on the prediction type
#                 if noise_scheduler.prediction_type == "epsilon":
#                     target = noise
#                 elif noise_scheduler.prediction_type == "v_prediction":
#                     target = noise_scheduler.get_velocity(latents, noise, timesteps)
#                 else:
#                     raise ValueError(f"Unknown prediction type {noise_scheduler.prediction_type}")
# 
#                 # Predict the noise residual and compute loss
# 
#                 model_pred = unet(noisy_latents, timesteps, encoder_hidden_states, skeleton=skeleton).sample
#                 loss = F.mse_loss(model_pred.float(), target.float(), reduction="mean")
# 
#                 # Gather the losses across all processes for logging (if we use distributed training).
#                 avg_loss = accelerator.gather(loss.repeat(train_batch_size)).mean()
#                 train_loss += avg_loss.item() / gradient_accumulation_steps
# 
#                 # Backpropagate (TODO)
#                 #accelerator.backward(loss)
#                 #if accelerator.sync_gradients:
#                     #accelerator.clip_grad_norm_(unet.parameters(), max_grad_norm)
#                 optimizer.step()
#                 lr_scheduler.step()
#                 optimizer.zero_grad()
# 
#             # Checks if the accelerator has performed an optimization step behind the scenes
# 
#             config_name = config_name.split('.')[0]
#             if accelerator.sync_gradients:
#                 progress_bar.update(1)
#                 global_step += 1
#                 accelerator.log({"train_loss": train_loss}, step=global_step)
#                 train_loss = 0.0
#                 logger.info(f"checkpoint: {checkpointing_steps}, global_step: {global_step} ")
#                 if global_step % checkpointing_steps == 0:
#                     if accelerator.is_main_process:
#                         save_path = os.path.join(output_dir, f"{config_name}-checkpoint-{global_step}")
#                         accelerator.save_state(save_path)
#                         logger.info(f"Saved state to {save_path}")
# 
#                 if global_step % validation_steps == 0:
#                     if accelerator.is_main_process:
#                         samples = []
#                         generator = torch.Generator(device=latents.device)
#                         generator.manual_seed(seed)
# 
#                         ddim_inv_latent = None
#                         if validation_data.use_inv_latent:
#                             inv_latents_path = os.path.join(output_dir, f"inv_latents/ddim_latent-{global_step}.pt")
#                             ddim_inv_latent = ddim_inversion(
#                                 validation_pipeline, ddim_inv_scheduler, video_latent=latents,
#                                 num_inv_steps=validation_data.num_inv_steps, prompt="")[-1].to(weight_dtype)
#                             torch.save(ddim_inv_latent, inv_latents_path)
# 
#                         for idx, prompt in enumerate(validation_data.prompts):
#                             sample = validation_pipeline(prompt, generator=generator, latents=ddim_inv_latent,
#                                                         skeleton_path=skeleton_path,
#                                                         **validation_data).videos
#                             save_videos_grid(sample, f"{output_dir}/samples/{config_name}-sample-{global_step}/{prompt}.gif")
#                             samples.append(sample)
#                         samples = torch.concat(samples)
#                         save_path = f"{output_dir}/samples/{config_name}-sample-{global_step}.gif"
#                         save_videos_grid(samples, save_path)
#                         logger.info(f"Saved samples to {save_path}")
#             logs = {"step_loss": loss.detach().item(), "lr": lr_scheduler.get_last_lr()[0]}
#             progress_bar.set_postfix(**logs)
# 
#             if global_step >= max_train_steps:
#                 break
# 
#     # Create the pipeline using the trained modules and save it.
#     accelerator.wait_for_everyone()
#     if accelerator.is_main_process:
#         unet = accelerator.unwrap_model(unet)
#         pipeline = FollowYourPosePipeline.from_pretrained(
#             pretrained_model_path,
#             text_encoder=text_encoder,
#             vae=vae,
#             unet=unet,
#         )
#         pipeline.save_pretrained(output_dir)
# 
#     accelerator.end_training()
# 
# 
# if __name__ == "__main__":
#     parser = argparse.ArgumentParser()
#     parser.add_argument("--config", type=str, default="./configs/pose.yaml")
#     args = parser.parse_args()
# 
#     main(**OmegaConf.load(args.config))
# 
#     # Replace config file with the generated one
#     # Specify the source YAML file and the destination YAML file
#     #source_file_path = "./output/config.yaml"
#     #destination_file_path = os.path.join("./configs/",OmegaConf.load(args.config).save_as)
#     #print(destination_file_path)
#     # Load the contents of the source YAML file
#     #with open(source_file_path, 'r') as source_file:
#     #    source_data = yaml.load(source_file, Loader=yaml.FullLoader)
# 
#     # Write the contents of the source YAML file to the destination YAML file
#     #with open(destination_file_path, 'w') as destination_file:
#     #    yaml.dump(source_data, destination_file, default_flow_style=False)
# 
#     #print("File contents replaced successfully.")
#

# Commented out IPython magic to ensure Python compatibility.
# #@title File creation
# %%writefile trainFYP.py
# import argparse
# import datetime
# import logging
# import inspect
# import yaml
# import math
# import os
# from typing import Dict, Optional, Tuple
# from omegaconf import OmegaConf
# from tqdm import tqdm
# import torch
# import torch.nn.functional as F
# import torch.utils.checkpoint
# 
# import diffusers
# import transformers
# from accelerate import Accelerator
# from accelerate.logging import get_logger
# from accelerate.utils import set_seed
# from diffusers import AutoencoderKL, DDPMScheduler, DDIMScheduler
# from diffusers.optimization import get_scheduler
# from diffusers.utils import check_min_version
# from diffusers.utils.import_utils import is_xformers_available
# from tqdm.auto import tqdm
# from transformers import CLIPTextModel, CLIPTokenizer
# 
# from followyourpose.models.unet import UNet3DConditionModel
# from followyourpose.data.dataloader import Dataset
# from followyourpose.pipelines.pipeline_followyourpose import FollowYourPosePipeline
# from followyourpose.util import save_videos_grid, ddim_inversion
# from einops import rearrange
# 
# 
# # Will error if the minimal version of diffusers is not installed. Remove at your own risks.
# check_min_version("0.10.0.dev0")
# 
# logger = get_logger(__name__, log_level="INFO")
# 
# 
# def main(
#     pretrained_model_path: str,
#     config_name: str,
#     output_dir: str,
#     train_data: Dict,
#     validation_data: Dict,
#     validation_steps: int = 100,
#     trainable_modules: Tuple[str] = (
#         "attn1.to_q",
#         "attn2.to_q",
#         "attn_temp",
#     ),
#     train_batch_size: int = 1,
#     max_train_steps: int = 500,
#     learning_rate: float = 3e-2,
#     scale_lr: bool = False,
#     lr_scheduler: str = "constant",
#     lr_warmup_steps: int = 0,
#     adam_beta1: float = 0.9,
#     adam_beta2: float = 0.999,
#     adam_weight_decay: float = 1e-2,
#     adam_epsilon: float = 1e-08,
#     max_grad_norm: float = 1.0,
#     gradient_accumulation_steps: int = 1,
#     gradient_checkpointing: bool = True,
#     checkpointing_steps: int = 500,
#     resume_from_checkpoint: Optional[str] = None,
#     mixed_precision: Optional[str] = "fp16",
#     use_8bit_adam: bool = False,
#     enable_xformers_memory_efficient_attention: bool = True,
#     seed: Optional[int] = None,
#     skeleton_path: Optional[str] = None,
# ):
#     *_, config = inspect.getargvalues(inspect.currentframe())
# 
#     accelerator = Accelerator(
#         gradient_accumulation_steps=gradient_accumulation_steps,
#         mixed_precision=mixed_precision,
#     )
# 
#     # Make one log on every process with the configuration for debugging.
#     logging.basicConfig(
#         format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
#         datefmt="%m/%d/%Y %H:%M:%S",
#         level=logging.INFO,
#     )
#     logger.info(accelerator.state, main_process_only=False)
#     if accelerator.is_local_main_process:
#         transformers.utils.logging.set_verbosity_warning()
#         diffusers.utils.logging.set_verbosity_info()
#     else:
#         transformers.utils.logging.set_verbosity_error()
#         diffusers.utils.logging.set_verbosity_error()
# 
#     # If passed along, set the training seed now.
#     if seed is not None:
#         set_seed(seed)
# 
#     # Handle the output folder creation
#     if accelerator.is_main_process:
# 
#         os.makedirs(output_dir, exist_ok=True)
#         os.makedirs(f"{output_dir}/samples", exist_ok=True)
#         os.makedirs(f"{output_dir}/inv_latents", exist_ok=True)
#         OmegaConf.save(config, os.path.join(output_dir, 'config.yaml'))
# 
#     # Load scheduler, tokenizer and models.
#     noise_scheduler = DDPMScheduler.from_pretrained(pretrained_model_path, subfolder="scheduler")
#     tokenizer = CLIPTokenizer.from_pretrained(pretrained_model_path, subfolder="tokenizer")
#     text_encoder = CLIPTextModel.from_pretrained(pretrained_model_path, subfolder="text_encoder")
#     vae = AutoencoderKL.from_pretrained(pretrained_model_path, subfolder="vae")
#     unet = UNet3DConditionModel.from_pretrained_2d(pretrained_model_path, subfolder="unet")
# 
#     # Freeze vae and text_encoder
#     vae.requires_grad_(False)
#     text_encoder.requires_grad_(False)
# 
#     unet.requires_grad_(False)
#     for name, module in unet.named_modules():
#         if name.endswith(tuple(trainable_modules)):
#             for params in module.parameters():
#                 params.requires_grad = True
# 
#     if enable_xformers_memory_efficient_attention:
#         if is_xformers_available():
#             unet.enable_xformers_memory_efficient_attention()
#         else:
#             raise ValueError("xformers is not available. Make sure it is installed correctly")
# 
#     if gradient_checkpointing:
#         unet.enable_gradient_checkpointing()
# 
#     if scale_lr:
#         learning_rate = (
#             learning_rate * gradient_accumulation_steps * train_batch_size * accelerator.num_processes
#         )
# 
#     # Initialize the optimizer
#     if use_8bit_adam:
#         try:
#             import bitsandbytes as bnb
#         except ImportError:
#             raise ImportError(
#                 "Please install bitsandbytes to use 8-bit Adam. You can do so by running `pip install bitsandbytes`"
#             )
# 
#         optimizer_cls = bnb.optim.AdamW8bit
#     else:
#         optimizer_cls = torch.optim.AdamW
# 
#     optimizer = optimizer_cls(
#         unet.parameters(),
#         lr=learning_rate,
#         betas=(adam_beta1, adam_beta2),
#         weight_decay=adam_weight_decay,
#         eps=adam_epsilon,
#     )
# 
#     # Get the training dataset
#     train_dataset = Dataset(accelerator=accelerator, **train_data)
# 
# 
#     # DataLoaders creation:
#     train_dataloader = torch.utils.data.DataLoader(
#         train_dataset, batch_size=train_batch_size
#     )
# 
#     # Get the validation pipeline
#     validation_pipeline = FollowYourPosePipeline(
#         vae=vae, text_encoder=text_encoder, tokenizer=tokenizer, unet=unet,
#         scheduler=DDIMScheduler.from_pretrained(pretrained_model_path, subfolder="scheduler")
#     )
#     validation_pipeline.enable_vae_slicing()
#     ddim_inv_scheduler = DDIMScheduler.from_pretrained(pretrained_model_path, subfolder='scheduler')
#     ddim_inv_scheduler.set_timesteps(validation_data.num_inv_steps)
# 
#     # Scheduler
#     lr_scheduler = get_scheduler(
#         lr_scheduler,
#         optimizer=optimizer,
#         num_warmup_steps=lr_warmup_steps * gradient_accumulation_steps,
#         num_training_steps=max_train_steps * gradient_accumulation_steps,
#     )
# 
#     # Prepare everything with our `accelerator`.
#     unet, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
#         unet, optimizer, train_dataloader, lr_scheduler
#     )
# 
#     # For mixed precision training we cast the text_encoder and vae weights to half-precision
#     # as these models are only used for inference, keeping weights in full precision is not required.
#     weight_dtype = torch.float32
#     if accelerator.mixed_precision == "fp16":
#         weight_dtype = torch.float16
#     elif accelerator.mixed_precision == "bf16":
#         weight_dtype = torch.bfloat16
# 
#     # Move text_encode and vae to gpu and cast to weight_dtype
#     text_encoder.to(accelerator.device, dtype=weight_dtype)
#     vae.to(accelerator.device, dtype=weight_dtype)
# 
#     # We need to recalculate our total training steps as the size of the training dataloader may have changed.
#     num_update_steps_per_epoch = math.ceil(len(train_dataloader) / gradient_accumulation_steps)
#     # Afterwards we recalculate our number of training epochs
#     num_train_epochs = math.ceil(max_train_steps / num_update_steps_per_epoch)
# 
#     # We need to initialize the trackers we use, and also store our configuration.
#     # The trackers initializes automatically on the main process.
#     if accelerator.is_main_process:
#         accelerator.init_trackers("text2video-fine-tune")
# 
#     # Train!
#     total_batch_size = train_batch_size * accelerator.num_processes * gradient_accumulation_steps
# 
#     logger.info("***** Running training *****")
#     logger.info(f"  Num examples = {len(train_dataset)}")
#     logger.info(f"  Num Epochs = {num_train_epochs}")
#     logger.info(f"  Instantaneous batch size per device = {train_batch_size}")
#     logger.info(f"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}")
#     logger.info(f"  Gradient Accumulation steps = {gradient_accumulation_steps}")
#     logger.info(f"  Total optimization steps = {max_train_steps}")
#     global_step = 0
#     first_epoch = 0
# 
#     # Potentially load in the weights and states from a previous save
#     if resume_from_checkpoint:
#         if resume_from_checkpoint != "latest":
#             path = os.path.basename(resume_from_checkpoint)
#         else:
#             # Get the most recent checkpoint
#             dirs = os.listdir(output_dir)
#             dirs = [d for d in dirs if d.startswith("checkpoint")]
#             dirs = sorted(dirs, key=lambda x: int(x.split("-")[1]))
#             path = dirs[-1]
#         accelerator.print(f"Resuming from checkpoint {path}")
#         accelerator.load_state(os.path.join(output_dir, path))
#         global_step = int(path.split("-")[1])
#         accelerator.print(f"Gloabl Step: {global_step}")
#         first_epoch = global_step // num_update_steps_per_epoch
#         accelerator.print(f"first_epoch: {first_epoch}")
#         resume_step = global_step % num_update_steps_per_epoch
#         accelerator.print(f"resume_step: {resume_step}")
# 
#     # Only show the progress bar once on each machine.
#     progress_bar = tqdm(range(global_step, max_train_steps), disable=not accelerator.is_local_main_process, unit="step", unit_scale=True, desc="Training Progress")
#     progress_bar.set_description("Steps")
# 
# 
# 
#     for epoch in range(first_epoch, num_train_epochs):
#         unet.train()
#         train_loss = 0.0
#         for step, batch in enumerate(train_dataloader):
#             # Skip steps until we reach the resumed step
#             if resume_from_checkpoint and epoch == first_epoch and step < resume_step:
#                 if step % gradient_accumulation_steps == 0:
#                     progress_bar.update(1)
#                 continue
# 
#             with accelerator.accumulate(unet):
#                 # Convert videos to latent space
#                 pixel_values = batch["pixel_values"].to(weight_dtype)
# 
#                 skeleton = batch["pose"].to(weight_dtype)
# 
# 
#                 video_length = pixel_values.shape[1]
# 
#                 pixel_values = rearrange(pixel_values, "b f c h w -> (b f) c h w")
#                 latents = vae.encode(pixel_values).latent_dist.sample()
#                 latents = rearrange(latents, "(b f) c h w -> b c f h w", f=video_length)
#                 latents = latents * 0.18215
# 
#                 # Sample noise that we'll add to the latents
#                 noise = torch.randn_like(latents)
#                 bsz = latents.shape[0]
#                 # Sample a random timestep for each video
#                 timesteps = torch.randint(0, noise_scheduler.num_train_timesteps, (bsz,), device=latents.device)
#                 timesteps = timesteps.long()
# 
#                 # Add noise to the latents according to the noise magnitude at each timestep
#                 # (this is the forward diffusion process)
#                 noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)
# 
#                 batch["prompt_ids"] = tokenizer(
#                     batch["sentence"], max_length=tokenizer.model_max_length, padding="max_length", truncation=True, return_tensors="pt"
#                 ).input_ids[0].to(accelerator.device).unsqueeze(0)
# 
#                 # Get the text embedding for conditioning
#                 encoder_hidden_states = text_encoder(batch["prompt_ids"])[0]
# 
#                 # Get the target for loss depending on the prediction type
#                 if noise_scheduler.prediction_type == "epsilon":
#                     target = noise
#                 elif noise_scheduler.prediction_type == "v_prediction":
#                     target = noise_scheduler.get_velocity(latents, noise, timesteps)
#                 else:
#                     raise ValueError(f"Unknown prediction type {noise_scheduler.prediction_type}")
# 
#                 # Predict the noise residual and compute loss
# 
#                 model_pred = unet(noisy_latents, timesteps, encoder_hidden_states, skeleton=skeleton).sample
#                 loss = F.mse_loss(model_pred.float(), target.float(), reduction="mean")
# 
#                 # Gather the losses across all processes for logging (if we use distributed training).
#                 avg_loss = accelerator.gather(loss.repeat(train_batch_size)).mean()
#                 train_loss += avg_loss.item() / gradient_accumulation_steps
# 
#                 # Backpropagate (TODO)
#                 #accelerator.backward(loss)
#                 if accelerator.sync_gradients:
#                     accelerator.clip_grad_norm_(unet.parameters(), max_grad_norm)
#                 optimizer.step()
#                 lr_scheduler.step()
#                 optimizer.zero_grad()
# 
#             # Checks if the accelerator has performed an optimization step behind the scenes
# 
#             config_name = config_name.split('.')[0]
#             if accelerator.sync_gradients:
#                 progress_bar.update(1)
#                 global_step += 1
#                 accelerator.log({"train_loss": train_loss}, step=global_step)
#                 train_loss = 0.0
#                 #logger.info(f"checkpoint: {checkpointing_steps}, global_step: {global_step} ")
#                 if global_step % checkpointing_steps == 0:
#                     if accelerator.is_main_process:
#                         save_path = os.path.join(output_dir, f"{config_name}-checkpoint-{global_step}")
#                         accelerator.save_state(save_path)
#                         logger.info(f"Saved state to {save_path}")
# 
#                 if global_step % validation_steps == 0:
#                     if accelerator.is_main_process:
#                         samples = []
#                         generator = torch.Generator(device=latents.device)
#                         generator.manual_seed(seed)
# 
#                         ddim_inv_latent = None
#                         if validation_data.use_inv_latent:
#                             inv_latents_path = os.path.join(output_dir, f"inv_latents/ddim_latent-{global_step}.pt")
#                             ddim_inv_latent = ddim_inversion(
#                                 validation_pipeline, ddim_inv_scheduler, video_latent=latents,
#                                 num_inv_steps=validation_data.num_inv_steps, prompt="")[-1].to(weight_dtype)
#                             torch.save(ddim_inv_latent, inv_latents_path)
# 
#                         for idx, prompt in enumerate(validation_data.prompts):
#                             sample = validation_pipeline(prompt, generator=generator, latents=ddim_inv_latent,
#                                                         skeleton_path=skeleton_path,
#                                                         **validation_data).videos
#                             save_videos_grid(sample, f"{output_dir}/samples/{config_name}-sample-{global_step}/{prompt}.gif")
#                             samples.append(sample)
#                         samples = torch.concat(samples)
#                         save_path = f"{output_dir}/samples/{config_name}-sample-{global_step}.gif"
#                         save_videos_grid(samples, save_path)
#                         logger.info(f"Saved samples to {save_path}")
# 
#             logs = {"step_loss": loss.detach().item(), "lr": lr_scheduler.get_last_lr()[0]}
#             progress_bar.set_postfix(**logs)
# 
#             if global_step >= max_train_steps:
#                 break
# 
#     # Create the pipeline using the trained modules and save it.
#     accelerator.wait_for_everyone()
#     if accelerator.is_main_process:
#         unet = accelerator.unwrap_model(unet)
#         pipeline = FollowYourPosePipeline.from_pretrained(
#             pretrained_model_path,
#             text_encoder=text_encoder,
#             vae=vae,
#             unet=unet,
#         )
#         pipeline.save_pretrained(output_dir)
# 
#     accelerator.end_training()
# 
# 
# if __name__ == "__main__":
#     parser = argparse.ArgumentParser()
#     parser.add_argument("--config", type=str, default="./configs/pose.yaml")
#     args = parser.parse_args()
# 
#     main(**OmegaConf.load(args.config))
# 
#     # Replace config file with the generated one
#     # Specify the source YAML file and the destination YAML file
#     #source_file_path = "./output/config.yaml"
#     #destination_file_path = os.path.join("./configs/",OmegaConf.load(args.config).save_as)
#     #print(destination_file_path)
#     # Load the contents of the source YAML file
#     #with open(source_file_path, 'r') as source_file:
#     #    source_data = yaml.load(source_file, Loader=yaml.FullLoader)
# 
#     # Write the contents of the source YAML file to the destination YAML file
#     #with open(destination_file_path, 'w') as destination_file:
#     #    yaml.dump(source_data, destination_file, default_flow_style=False)
# 
#     #print("File contents replaced successfully.")
#

rm -r ./configs/pose_train.yaml

rm -r  ./configs/pose_sample.yaml

#@title 2.9 Run Training
import os
import glob


# List YAML files in the directory
config_files = glob.glob(os.path.join(CONFIG_DIR, '*.yaml'))

# Iterate through the list of configuration files in CONFIG_NAME
for config_file in config_files:
    print("Processing: ", config_file)
    # Construct the command using the current configuration file
    command = f"TORCH_DISTRIBUTED_DEBUG=DETAIL python trainFYP.py --config=./{config_file}"

    # Execute the command for the current configuration file
    !{command}

#@title Display models
from ipywidgets import Checkbox, HBox, Button
from IPython.display import display, HTML
import base64

# Function to display GIFs
def display_gif(gif_path_or_url, model_name):
    if gif_path_or_url.startswith("http"):
        gif_html = f'<img src="{gif_path_or_url}" alt="GIF">'
    else:
        with open(gif_path_or_url, "rb") as f:
            data = f.read()
        data_uri = "data:image/gif;base64," + base64.b64encode(data).decode("utf-8")
        gif_html = f'<img src="{data_uri}" alt="GIF"><br>{model_name}'

    display(HTML(gif_html))

# Function to display GIFs horizontally
def display_gifs_horizontally(gif_paths, model_names):
    gif_html = ''
    for gif_path, model_name in zip(gif_paths, model_names):
        if gif_path.startswith("http"):
            gif_html += f'<div style="display: inline-block; margin: 10px;">'
            gif_html += f'<img src="{gif_path}" alt="GIF"><br>{model_name}'
            gif_html += '</div>'
        else:
            with open(gif_path, "rb") as f:
                data = f.read()
            data_uri = "data:image/gif;base64," + base64.b64encode(data).decode("utf-8")
            gif_html += f'<div style="display: inline-block; margin: 10px;">'
            gif_html += f'<img src="{data_uri}" alt="GIF"><br>{model_name}'
            gif_html += '</div>'

    display(HTML(gif_html))



# List saved models
def list_saved_models():
    # List saved models in the current directory by searching for the string "checkpoint"
    search_string = "checkpoint"
    folder = CONFIG_DIR  # Use to get the current directory
    saved_models = [f for f in os.listdir(folder)]
    return saved_models

# List saved GIFs
def list_saved_gifs():
    folder = f'{OUTPUT_DIR}/samples'  # Use to get the current directory
    saved_gifs = [f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f)) and f.lower().endswith(".gif")]
    return saved_gifs

models = list_saved_models()

# Create a list of Checkbox widgets
checkboxes = [Checkbox(value=False, description=model) for model in models]

# Display the checkboxes vertically
checkbox_widget = HBox(checkboxes)
display(checkbox_widget)

# Function to get the values of checked checkboxes
def get_checked_values(b):
    checked_values = [item for item, checkbox in zip(models, checkboxes) if checkbox.value]

    gif_files = list_saved_gifs()
    # Extract the base names of GIF files (without the ".gif" extension)
    gif_base_names = [os.path.splitext(gif)[0] for gif in gif_files]


    # Create a dictionary to store the highest values for each base name
    highest_values = {}

    # Iterate through GIF files and update the highest value in the dictionary
    for gif_file in gif_files:
        base_name = os.path.splitext(gif_file)[0]
        base_name_parts = base_name.split('-')
        base_name = '-'.join(base_name_parts[:-1])  # Remove the value part
        value = int(base_name_parts[-1])

        if base_name not in highest_values or value > highest_values[base_name]:
            highest_values[base_name] = value

    # Filter GIF files to keep only the highest values
    filtered_gif_files = []
    for gif_file in gif_files:
        base_name = os.path.splitext(gif_file)[0]
        base_name_parts = base_name.split('-')
        base_name = '-'.join(base_name_parts[:-1])  # Remove the value part
        value = int(base_name_parts[-1])

        if value == highest_values.get(base_name, value) and f"{base_name.split('-')[0]}.yaml" in checked_values:
            # display_gif(f"{OUTPUT_DIR}/samples/{gif_file}", f"{base_name.split('-')[0]}.yaml") # GIF URL or local path
            filtered_gif_files.append(f"{OUTPUT_DIR}/samples/{gif_file}")
    display_gifs_horizontally(filtered_gif_files,checked_values)


# Create a button to trigger the check
button = Button(description="Analyze")
button.on_click(get_checked_values)
display(button)

#@markdown Keep best model and remove unused models
import os
from ipywidgets import Dropdown, Button, Output
import shutil  # Import the shutil module for file deletion

# List saved models
def list_saved_models():
    # List saved models in the current directory by searching for the string "checkpoint"
    search_string = "checkpoint"
    folder = OUTPUT_DIR  # Use to get the current directory
    saved_models = [f for f in os.listdir(folder) if search_string in f]
    return saved_models

# Delete unused models
def delete_unused_models(user_selected_model, available_models):
    print(f"Keeping: {user_selected_model}")
    for model in available_models:
        if model != user_selected_model:
            model_path = os.path.join(OUTPUT_DIR, model)
            if os.path.exists(model_path):
                shutil.rmtree(model_path)
                print(f"Deleted: {model}")
    file_gifs = list_saved_gifs(user_selected_model)
    delete_samples(file_gifs)

# List all gifs
def list_saved_gifs(model):
    folder = f"{OUTPUT_DIR}/samples"  # Use to get the current directory
    search_string = f"{model.split('-')[0]}-sample-{model.split('-')[2]}"
    saved_gifs = [f for f in os.listdir(folder) if search_string not in f]
    return saved_gifs

# Delete samples of unused models
def delete_samples(samplesArr):
    for sample in samplesArr:
        sample_split = sample.split('.')
        # Remove gif file
        if(len(sample_split) == 2 and sample_split[1] == "gif"):
            file_path = f"{OUTPUT_DIR}/samples/{sample}"
            if os.path.exists(file_path):
                os.remove(file_path)
                print(f"{file_path} has been removed.")
            else:
                print(f"{file_path} does not exist.")
        # Remove gif folder
        else:
            folder_path = f"{OUTPUT_DIR}/samples/{sample}"
            if os.path.exists(folder_path):
                shutil.rmtree(folder_path)
                print(f"{folder_path} has been removed.")
            else:
                print(f"{folder_path} does not exist.")


# WIP

out = Output()

# UI components
dropdown = Dropdown(
    options=list_saved_models(),
    description='Select best model and remove unused models:'
)

selected_model = dropdown.value

button = Button(description="Select Best Model")


# Function to handle best model selection and deletion
def handle_best_model_selection(b):
    with out:
        available_models = list_saved_models()
        if not available_models:
            print("No models found.")
            return

        # user_selected_model = select_best_model(available_models)
        delete_unused_models(selected_model, available_models)

        print("All other artifacts have been deleted.")

button.on_click(handle_best_model_selection)

# Display UI components
display(dropdown, button, out)

"""## 3. Inferencing"""

#@title Select model config for inferencing

from ipyfilechooser import FileChooser
from IPython.display import display

# Create a FileChooser widget
file_chooser = FileChooser()
file_chooser.title = 'Select a model config'
file_chooser.default_path = "/content/ict3104-team17-2023/configs"  # Set your default directory path

# Set a filter to show only YAML files
file_chooser.filter_pattern = ['*.yaml', '*.yml']

# Define a function to capture the selected file
def on_file_selected(chooser):
    global SELECTED_CONF
    SELECTED_CONF = chooser.selected
    print(f"Selected file: {SELECTED_CONF}")

# Attach the event handler to the file chooser
file_chooser.register_callback(on_file_selected)

# Display the file chooser widget
display(file_chooser)

print(SELECTED_CONF)

# Commented out IPython magic to ensure Python compatibility.
# %%writefile txt2video.py
# 
# import argparse
# import datetime
# import logging
# import inspect
# import math
# import os
# from typing import Dict, Optional, Tuple
# from omegaconf import OmegaConf
# 
# import torch
# import torch.nn.functional as F
# import torch.utils.checkpoint
# 
# import diffusers
# import transformers
# from accelerate import Accelerator
# from accelerate.logging import get_logger
# from accelerate.utils import set_seed
# from diffusers import AutoencoderKL, DDPMScheduler, DDIMScheduler
# from diffusers.optimization import get_scheduler
# from diffusers.utils import check_min_version
# from diffusers.utils.import_utils import is_xformers_available
# from tqdm.auto import tqdm
# from transformers import CLIPTextModel, CLIPTokenizer
# 
# from followyourpose.models.unet import UNet3DConditionModel
# from followyourpose.data.hdvila import HDVilaDataset
# from followyourpose.pipelines.pipeline_followyourpose import FollowYourPosePipeline
# from followyourpose.util import save_videos_grid, ddim_inversion
# from einops import rearrange
# from typing import Dict, Optional, List, Any
# 
# 
# import sys
# sys.path.append('FollowYourPose')
# 
# # Will error if the minimal version of diffusers is not installed. Remove at your own risks.
# check_min_version("0.10.0.dev0")
# 
# logger = get_logger(__name__, log_level="INFO")
# 
# def main(
#     pretrained_model_path: str,
#     output_dir: str,
#     resume_from_checkpoint: str,
#     config_name: str,
#     train_data: Dict[str, Any],
#     validation_data: Dict[str, Any],
#     learning_rate: float,
#     scale_lr: bool,
#     lr_scheduler: str,
#     lr_warmup_steps: int,
#     adam_beta1: float,
#     adam_beta2: float,
#     adam_weight_decay: float,
#     adam_epsilon: float,
#     max_grad_norm: float,
#     train_batch_size: int,
#     max_train_steps: int,
#     gradient_accumulation_steps: int,
#     checkpointing_steps: int,
#     validation_steps: int,
#     trainable_modules: List[str],
#     seed: int,
#     mixed_precision: str,
#     use_8bit_adam: bool,
#     gradient_checkpointing: bool,
#     enable_xformers_memory_efficient_attention: bool,
#     skeleton_path: str
# ):
#     # The rest of the function remains the same
# 
# 
#     *_, config = inspect.getargvalues(inspect.currentframe())
# 
#     accelerator = Accelerator(
#         gradient_accumulation_steps=gradient_accumulation_steps,
#         mixed_precision=mixed_precision,
#     )
# 
#     # Make one log on every process with the configuration for debugging.
#     logging.basicConfig(
#         format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
#         datefmt="%m/%d/%Y %H:%M:%S",
#         level=logging.INFO,
#     )
#     logger.info(accelerator.state, main_process_only=False)
#     if accelerator.is_local_main_process:
#         transformers.utils.logging.set_verbosity_warning()
#         diffusers.utils.logging.set_verbosity_info()
#     else:
#         transformers.utils.logging.set_verbosity_error()
#         diffusers.utils.logging.set_verbosity_error()
# 
#     # If passed along, set the training seed now.
#     if seed is not None:
#         set_seed(seed)
# 
#     # Handle the output folder creation
#     if accelerator.is_main_process:
#         # now = datetime.datetime.now().strftime("%Y-%m-%dT%H-%M-%S")
#         # output_dir = os.path.join(output_dir, now)
#         os.makedirs(output_dir, exist_ok=True)
#         os.makedirs(f"{output_dir}/samples", exist_ok=True)
#         os.makedirs(f"{output_dir}/inv_latents", exist_ok=True)
#         OmegaConf.save(config, os.path.join(output_dir, 'config.yaml'))
# 
#     # Load scheduler, tokenizer and models.
#     noise_scheduler = DDPMScheduler.from_pretrained(pretrained_model_path, subfolder="scheduler")
#     tokenizer = CLIPTokenizer.from_pretrained(pretrained_model_path, subfolder="tokenizer")
#     text_encoder = CLIPTextModel.from_pretrained(pretrained_model_path, subfolder="text_encoder")
#     vae = AutoencoderKL.from_pretrained(pretrained_model_path, subfolder="vae")
#     unet = UNet3DConditionModel.from_pretrained_2d(pretrained_model_path, subfolder="unet")
# 
#     # Freeze vae and text_encoder
#     vae.requires_grad_(False)
#     text_encoder.requires_grad_(False)
# 
#     unet.requires_grad_(False)
# 
#     if enable_xformers_memory_efficient_attention:
#         if is_xformers_available():
#             unet.enable_xformers_memory_efficient_attention()
#         else:
#             raise ValueError("xformers is not available. Make sure it is installed correctly")
# 
#     if gradient_checkpointing:
#         unet.enable_gradient_checkpointing()
# 
# 
#     # Get the validation pipeline
#     validation_pipeline = FollowYourPosePipeline(
#         vae=vae, text_encoder=text_encoder, tokenizer=tokenizer, unet=unet,
#         scheduler=DDIMScheduler.from_pretrained(pretrained_model_path, subfolder="scheduler")
#     )
#     validation_pipeline.enable_vae_slicing()
#     ddim_inv_scheduler = DDIMScheduler.from_pretrained(pretrained_model_path, subfolder='scheduler')
#     ddim_inv_scheduler.set_timesteps(validation_data.num_inv_steps)
# 
#     unet = accelerator.prepare(unet)
#     # For mixed precision training we cast the text_encoder and vae weights to half-precision
#     # as these models are only used for inference, keeping weights in full precision is not required.
#     weight_dtype = torch.float32
#     if accelerator.mixed_precision == "fp16":
#         weight_dtype = torch.float16
#     elif accelerator.mixed_precision == "bf16":
#         weight_dtype = torch.bfloat16
# 
#     # Move text_encode and vae to gpu and cast to weight_dtype
#     text_encoder.to(accelerator.device, dtype=weight_dtype)
#     vae.to(accelerator.device, dtype=weight_dtype)
# 
#     # We need to recalculate our total training steps as the size of the training dataloader may have changed.
# 
#     # We need to initialize the trackers we use, and also store our configuration.
#     # The trackers initializes automatically on the main process.
#     if accelerator.is_main_process:
#         accelerator.init_trackers("text2video-fine-tune")
# 
#     global_step = 0
#     first_epoch = 0
# 
#     # Potentially load in the weights and states from a previous save
#     load_path = None
#     if resume_from_checkpoint:
#         if resume_from_checkpoint != "latest":
#             load_path = resume_from_checkpoint
#             output_dir = os.path.abspath(os.path.join(resume_from_checkpoint, ".."))
#         accelerator.print(f"load from checkpoint {load_path}")
#         accelerator.load_state(load_path)
# 
#         global_step = int(load_path.split("-")[-1])
# 
# 
#     if accelerator.is_main_process:
#         samples = []
#         generator = torch.Generator(device=accelerator.device)
#         generator.manual_seed(seed)
# 
#         ddim_inv_latent = None
# 
#         from datetime import datetime
# 
#         now = str(datetime.now())
#         # print(now)
#         for idx, prompt in enumerate(validation_data.prompts):
#             sample = validation_pipeline(prompt, generator=generator, latents=ddim_inv_latent,
#                                         skeleton_path=skeleton_path,
#                                         **validation_data).videos
#             save_videos_grid(sample, f"{output_dir}/inference/sample-{global_step}-{str(seed)}-{now}/{prompt}.gif")
#             samples.append(sample)
#         samples = torch.concat(samples)
#         save_path = f"{output_dir}/inference/sample-{global_step}-{str(seed)}-{now}.gif"
#         save_videos_grid(samples, save_path)
#         logger.info(f"Saved samples to {save_path}")
# 
# 
# 
# if __name__ == "__main__":
#     parser = argparse.ArgumentParser()
#     parser.add_argument("--config", type=str)
#     args = parser.parse_args()
#     main(**OmegaConf.load(args.config))

# Commented out IPython magic to ensure Python compatibility.
# %%writefile ./output/config.yaml
# 
# pretrained_model_path: ./checkpoints/stable-diffusion-v1-4
# config_name: apose1.yaml
# output_dir: ./output
# train_data:
#   meta_path: Charades_v1_desc.csv
#   prompt: A person walks into a garage and throws something into a bin. That same
#     person then picks up an open laptop off a shelf, closes it and walks way.;A person
#     runs into the garage, throws something in a bucket, picks up a laptop from a shelf,
#     closes it and then runs back out.
#   n_sample_frames: 4
#   width: 512
#   height: 512
#   sample_start_idx: 0
#   dataset_set: train
#   sample_frame_rate: 4
# validation_data:
#   prompts:
#   - A Iron man on the beach,
#   video_length: 4
#   width: 512
#   height: 512
#   num_inference_steps: 20
#   guidance_scale: 12.5
#   use_inv_latent: false
#   num_inv_steps: 20
#   dataset_set: val
# validation_steps: 1050
# trainable_modules:
# - attn1.to_q
# - attn2.to_q
# - attn_temp
# - conv_temporal
# train_batch_size: 1
# max_train_steps: 1050
# learning_rate: 3.0e-05
# scale_lr: false
# lr_scheduler: constant
# lr_warmup_steps: 0
# adam_beta1: 0.9
# adam_beta2: 0.999
# adam_weight_decay: 0.01
# adam_epsilon: 1.0e-08
# max_grad_norm: 1.0
# gradient_accumulation_steps: 1
# gradient_checkpointing: true
# checkpointing_steps: 1050
# resume_from_checkpoint: ./output/apose2-checkpoint-1050
# mixed_precision: 'no'
# use_8bit_adam: false
# enable_xformers_memory_efficient_attention: true
# seed: 33
# skeleton_path: ./mmpose_output/output_file.mp4
#

#@title Begin inferencing

#Checks for the selected config in code previously
if SELECTED_CONF:
  !nvidia-smi
  !TORCH_DISTRIBUTED_DEBUG=DETAIL accelerate launch txt2video.py --config=$SELECTED_CONF
else:
  print("No config file selected")

#@title Display inferencing results (Hard coded needs fixing)

import ipywidgets as widgets
from IPython.display import display
import os
import configparser
import cv2

if_name_input = widgets.Text(
    value='',
    placeholder='Enter inferenced video name',
    description='Inferenced Video name:'
)

video_name_input = widgets.Text(
    value='',
    placeholder='Enter video name',
    description='New Video name:'
)


caption_input = widgets.Text(
    value='',
    placeholder='Enter caption',
    description='Caption:'
)

save_button = widgets.Button(
    description="Save Video and Caption"
)

output = widgets.Output()


def save_video(video_name,inf_name):
    # Define the output path for the video file
    video_output_path = f'/content/ict3104-team17-2023/mmpose_input/{video_name}.mp4'

    # Open the input video file (you should provide the path to your input video)
    input_video_path = f'/content/ict3104-team17-2023/mmpose_input/{inf_name}.mp4'
    cap = cv2.VideoCapture(input_video_path)

    # Get video properties
    frame_width = int(cap.get(3))
    frame_height = int(cap.get(4))
    frame_rate = int(cap.get(5))

    # Define the codec and create a VideoWriter object
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for MP4 format
    out = cv2.VideoWriter(video_output_path, fourcc, frame_rate, (frame_width, frame_height))

    # Read frames from the input video and write them to the output video
    while True:
        ret, frame = cap.read()
        if not ret:
            break

        out.write(frame)

    # Release video objects
    cap.release()
    out.release()

def save_caption_to_config(caption_text, config_file_path):
    config = configparser.ConfigParser()
    config.add_section('Captions')
    config.set('Captions', 'caption_text', caption_text)
    with open(config_file_path, 'w') as configfile:
        config.write(configfile)

def save_video_and_caption(_):
    inf_name = if_name_input.value
    video_name = video_name_input.value
    caption_text = caption_input.value


    # Define the output path for the INI file
    config_file_path = f'/content/ict3104-team17-2023/mmpose_input/{video_name}.ini'

    # Save the caption to the INI file
    save_caption_to_config(caption_text, config_file_path)

    # Save the video in .mp4 format
    save_video(video_name,inf_name)

    # Display a message in the output widget to indicate that both the video and caption have been saved
    with output:
        print(f"Saved video to file: {video_name}.mp4")
        print(f"Saved caption to INI file: {config_file_path}")

save_button.on_click(save_video_and_caption)

widget_container = widgets.VBox([if_name_input,video_name_input, caption_input, save_button, output])

display(widget_container)

#@title Attach caption to video

import os
import subprocess
from IPython.display import display, Video
import configparser

# Specify the folder where the videos with captions are saved
output_folder = "/content/ict3104-team17-2023/mmpose_input/"

# List all video files in the output folder
video_files = [file for file in os.listdir(output_folder) if file.endswith(".mp4")]

# Specify the fixed video size (e.g., 300x300)
fixed_video_width = 300
fixed_video_height = 300

# Specify the fixed frame size (e.g., 350x400)
fixed_frame_width = 350
fixed_frame_height = 400

# Specify the caption offset (e.g., 50 pixels above the bottom)
caption_offset = 50

# Iterate through the video files
for video_file in video_files:
    input_video_path = os.path.join(output_folder, video_file)
    output_video_path = os.path.join(output_folder, f"{video_file.split('.')[0]}_with_caption.mp4")

    # Read the config file
    config_file_path = os.path.join(output_folder, f"{video_file.split('.')[0]}.ini")
    if os.path.exists(config_file_path):
        config = configparser.ConfigParser()
        config.read(config_file_path)
        caption_text = config.get("Captions", "caption_text")
    else:
        caption_text = "No caption available"

    # Use FFmpeg to resize the video to the fixed dimensions and overlay the caption text at the bottom
    ffmpeg_command = (
        "ffmpeg -i {input_video} -vf "
        "\"scale={video_width}:{video_height},"
        "drawtext=text='{caption}':x=10:y=h-{caption_offset}:fontsize=24:fontcolor=white,"
        "scale={frame_width}:{frame_height}\" "
        "-c:v libx264 -preset medium -crf 23 -c:a aac -strict experimental -b:a 192k {output_video}"
    )

    subprocess.run(
        ffmpeg_command.format(
            input_video=input_video_path,
            caption=caption_text,
            video_width=fixed_video_width,
            video_height=fixed_video_height,
            caption_offset=caption_offset,
            frame_width=fixed_frame_width,
            frame_height=fixed_frame_height,
            output_video=output_video_path,
        ),
        shell=True,
    )

    print(f"Processed {video_file} with caption from config file.")

#@title Combine skeleton to inference video and display (Hard coded needs fixing)
from moviepy.editor import VideoFileClip, clips_array
from IPython.display import display, HTML
import base64

# Function to combine 2 video files into 1 side by side
def combine_video_files(skeleton_vid_wif_caption,inference_vid):
    video1 = VideoFileClip(skeleton_vid_wif_caption)
    video2 = VideoFileClip(inference_vid)

    # Get the dimensions (height and width) of the smaller video
    min_height = min(video1.h, video2.h)
    min_width = min(video1.w, video2.w)

    # Resize both videos to have the same height and width
    video1 = video1.resize(height=min_height, width=min_width)
    video2 = video2.resize(height=min_height, width=min_width)


    # Combine the videos side by side
    final_video = clips_array([[video1, video2]])
    # Write the final video to an output file
    final_video.write_videofile("/content/ict3104-team17-2023/pose_input/combined_video.mp4", codec="libx264")



def convert_video_to_gif(vid):
    # Load the MP4 video
    video = VideoFileClip(vid)
    # Convert the video to a GIF
    video.write_gif("/content/ict3104-team17-2023/pose_input/output.gif", fps= 10)
    # Close the video file
    video.close()
    return "/content/ict3104-team17-2023/pose_input/output.gif"

# Function to display GIFs
def display_gif(gif_path_or_url):
    if gif_path_or_url.startswith("http"):
        gif_html = f'<img src="{gif_path_or_url}" alt="GIF">'
    else:
        with open(gif_path_or_url, "rb") as f:
            data = f.read()
        data_uri = "data:image/gif;base64," + base64.b64encode(data).decode("utf-8")
        gif_html = f'<img src="{data_uri}" alt="GIF">'

    display(HTML(gif_html))

combine_video_files("/content/ict3104-team17-2023/pose_input/Testt_with_caption.mp4","/content/ict3104-team17-2023/pose_input/jumping1.mp4")
display_gif(convert_video_to_gif("/content/ict3104-team17-2023/pose_input/combined_video.mp4"))